---
title: "Introduction to Data Structures & Complexity"
format:
   html:
    toc: true
    toc-depth: 2
    number-sections: true
    code-block-bg: true
    code-block-border-left: true
    
---

A data structure is a way of storing data in a computer so that it can be used efficiently. In your programming experience so far, you have used some data structures but may not have known that they were data structures. An array is an example of a data structure. The elements are arranged in contiguous memory locations so that they can be accessed, stored, and manipulated quickly. Other types of data structures include sets, unions, records, graphs, trees, etc. Do not confuse data types and data structures. Think of data types as atoms and data structures as molecules. Examples of data types are int, short, long, float, double, char, boolean, etc. So we've said that we store data in data structures so that it can be manipulated efficiently, but what exactly does that mean? Manipulation usually means searching, sorting, changing, etc. We normally process some form of input efficiently to generate the “right” output. Data structures allow us to store this data in a way that makes the processing better. 

Data structures is essentially what this class is about (as I have mentioned before on many occasions). We will spend the rest of this quarter looking at different commonly used data structures and how we can use them in our code.

# Complexity
Computer Scientists are weird people. Whereas the rest of the world is looking at the best times as a measure of effectiveness and excellence, we are looking at the worst time. Usain bolt is celebrated as a runner because he run 100m in 9.58s. In fact, every four years, the whole world stops to watch and celebrate different people demonstrate the best of the best when it comes to physical feats. In computer science, we are more interested in the worst case. This is partly because knowing the worst case allows us to give hard guarantees in our estimates, and prepare for deadlines. After all, the saying is “hope for the best, prepare for the worst.” 

To represent this “worst case” we use what is referred to as a **big-O notation** as a measure of how well or poor an algorithm is. This might seem counter intuitive since run time seems the natural unit to measure the performance of an algorithm. However, while run time is a very accurate time measure, it is a very inaccurate algorithm comparison measure. This is because A LOT of factors affect the runtime of an algorithm and many of them have nothing to do with the algorithm itself which is what we are interested in.

For example, if I gave you a program to sort some numbers and after executing it, you determined that it took 5 seconds to run. That 5 seconds cannot tell you whether it is better than another sorting program that your colleague executed in 8 seconds. Why? How much of that 5 seconds run time is down to the computer system one is using (faster processor)? Or the programming language that it was coded in? Or the skill level of the programmer? Or the specific state of the list that was to be sorted (since it might take a short time for a list that was already sorted)? Or the size of the list that was sorted? Almost all these factors that affect the runtime have nothing to do with the “goodness” or “badness” of the actual algorithm which is what we are interested in in order to compare it with another algorithm.

Big-O is the computer scientist’s answer to all these problems. It is inexact by design and it is this feature that typically confuses people who are looking at big-O for the first time. In a bid to ignore all the non- factors we mentioned earlier, big-O describes the performance of an algorithm in terms of a variable (typically n) that represents the size of the problem. In fact, the big-O notation shows how dependent
the algorithms runtime would be on the size of the input data. This data could be anything from the size of a string, to the size of an array, to the size of the problem space, to the number of guests, to the number of slices of pizza. This allows us to shed the fine details and divide problems into categories or broad classes based entirely on how they perform as the size of the data is increased.

# Dinner Party
As an example, let us consider this scenario [1]. You are preparing to host a dinner party and this task involves a lot of algorithms which we shall discuss and then categorize using their big-O notation (i.e. how they perform as the data size increases). 

The first task on your to-do list is cleaning your house. This task does not depend on the number of guests that we are inviting to the dinner party (n). It could take either 3 days or 3 hours (depending on the size of your house) but this is of no consequence to a computer scientist. Since this problem does not change its runtime when the number of guests changes, it is referred to as a **constant time problem** or $O(1)$.Notice that the variable n does not appear in the parentheses because its run time does not depend on n.

When your guests are at the table, another task on your to-do list is to pass the plate of hors d'oeuvres (its a fancy dinner party) around the table. Clearly the amount of time that this task will take will depend directly on the number of guests at the party. If its just 3 people, it should take about half the amount of time as if it was 6 people. And a dinner party of 300 people would take 100 times as long as the 3. This kind of problem is referred to as a **linear** problem or $O(n)$. Because big-O is inexact, we don’t care about any multiplicative factors. In the context of this example, we don’t care whether the dish has to be passed round twice or once. To big-O, these are both the same i.e. linear because whether you pass the dish around once or twice, the runtime will grow linearly with the number of guests you invite.

In fact, adding a constant subtask does not change its big-O. Making an announcement (which is O(1)) before passing round the hors d'oeuvres (O(n)) still remains O(n) because the runtime for the combined task will still be linear with respect to the number of guests invited. Making an announcement and then passing the dish around a table of 10 will still be about half as short as making an announcement and then passing the dish around a table of 20. This ignoring of less important factors is particularly difficult to wrap your mind around when the constant time factor is a big factor. For example, remodeling your kitchen (O(1)) and then passing the dish around is still in the same category as just passing the dish around even though the first one will take significantly longer. They are in the same category because as the size of the data increases (n), they respond in the same way i.e. linearly.

Another task could be having every guest hug each other when they arrive. If there are just two guests, only one hug would be required. With 10 guests, 45 hugs would be required. With 50 guests, 1225 hugs. With 100 guests, 4950 guests. It is easy to tell that the runtime of this problem grows at a faster rate than the size of the problem i.e. the number of hugs becomes larger at a faster rate than that at which the number of guests grows. In fact, this problem falls in the **polynomial** category; more specifically the **quadratic** kind $O(n^2)$. This means that if the  nput data size doubles, the runtime will increase by a factor of 4. Again any multiplicative or smaller factors don’t matter in big-O. For example, if your task involved everyone hugging each other, then passing a dish around the table, as well as a speech at the beginning of the dinner, the whole task would be $O(n^2)$ because of all the subtasks, it is the $O(n^2)$ factor that will dominate the runtime of the algorithm (i.e. it is the task that will grow most significantly as the input data size increases). Recall that big-O is inexact. We are only trying to capture an expression that represents how its runtime will grow with an increasing data size. Imagine that your dinner guests got involved in a game that required you to stand them in order of height for a game. The sorting algorithms you are familiar with from the Living With Cyber series (e.g. bubble sort) would fall into the O(n2) family. Sorting a group of 100 people will take 100 times as long as sorting a group of 10 people even if the size of the group has only increased by a factor of 10. 

Suppose that as dinner host, your roles involve having short conversations with not just everyone individually, but every possible grouping of your guests. For example with 3 guests, A, B and C. You have to have a conversation with A, B, C, AB, AC, BC, and ABC (i.e. 7 conversations). You don’t need to be an introvert to be daunted by such a task because it grows even faster than the quadratic category discussed earlier. With 4 guests (ABCD), you would need to have 15 conversations i.e. A, B, C, D, AB, AC, AD, BC, BD, CD, ABC, ABD, ACD, BCD, and ABCD. Notice how adding a single guest has basically doubled the amount of work you have to do. It would require 31 conversations for 5 guests, and 63 for 6 guests. This category of problems is **exponential** or **$O(2^n)$**, and algorithms in this category are typically avoided because their runtimes get very ugly even for relatively small data sizes. You might recall that we discussed the towers of Hanoi in the Living with Cyber series. The best solution to Towers of Hanoi falls in this category. 

One other task as dinner host could be deciding how to seat all your guests. Perhaps your guests are very particular about how they should be seated, who they seat next to, etc. Figuring out a seating arrangement even with basic limitations might involve enumerating all the different seating possibilities, and then ranking them and choosing the best. For example, with three guests (ABC), they could be sat as ABC, ACB, BAC, BCA, CBA or CAB. This means that with just 3 guests, I have to write out and compare 6 different seating arrangements. With 4 guests (ABCD), they could be arranged as 'ABCD', 'ABDC', 'ACBD', 'ACDB', 'ADBC', 'ADCB', 'BACD', 'BADC', 'BCAD', 'BCDA', 'BDAC', 'BDCA', 'CABD', 'CADB', 'CBAD', 'CBDA', 'CDAB', 'CDBA', 'DABC', 'DACB', 'DBAC', 'DBCA', 'DCAB', or 'DCBA' for a grand total of 24 arrangements. This category is perhaps the worst of the worst and is referred to as **factorial** or **$O(n!)$**. This explains why making seating arrangements at weddings is a very stressful and time intensive venture. 

One last task. Supposed that in the game we mentioned earlier (where your guests are standing in order of height), you have a task of figuring out whom among your guests has a specific height (which is given in units you are unfamiliar with but which your guests are comfortable with). A quick approach (which should also be familiar to you) would be to go to the middle of the line, and ask that guest if he/ she is the height you’re looking for. It should also be easy to determine whether he/she is taller or shorter than the required height, and as a result you will discard half your guest list from possible suspects. This is similar to the binary search algorithm that you are familiar with. As discussed in Living with Cyber, this algorithm belongs in the **logarithmic** category or **$O(log_2n)$**. Algorithms in this category typically reduce the size of the data at each step (by half in this case). 

There is another major category called the **quasi-linear / linearithmic** or **$O(nlog_2n)$**. It doesn’t grow as fast as O(n2) but is faster than O(n). There are a few sorting algorithms that fall into this category that we will be discussing later in the course. 

Broadly speaking, those are the main big-O categories. Once you understand them, you should be able to look at different algorithms and determine which group or family they belong to. Sometimes this involves looking at the algorithm itself, and other times it involves looking at a mathematical expression for its runtime, $T(n)$ given in terms of its data size, $n$.
Can you describe the order of growth from these functions using the big-O notation?\
$$n^3 +2n$$                   $$2n^3 + n^2$$
$$n+2nlog(n)$$                $$3n +2n^7$$
$$n(n+3)/2$$                  $$6$$
$$1+n^2 +2n^3+3n^4+2^n$$      $$n^3+2n log(n)$$

::: {.callout-note}
Remember the trick is to pick the dominant term and ignore the constants. Basically which term will get out of hand as n increases to really large values?
:::


# Starting with the algorithm
Keeping with food, let’s look at a few algorithms you must have used at some point in your lives [2].
```{.default}
public donut takeDonut(donut [] box)
{
    return box[0]; //return the donut in the first position of donut box
}
```

What complexity do you think the algorithm above demonstrates? Does it depend on the number of donuts in the box?

```{.default}
public void eatFries(fries [] box)
{
    for(int i = 0; i < box.length; i++)
    {
        //dip chip in ketchup
        //eat fry
    }
}
```
What about the complexity of the algorithm above? What kind of relationship is there between its runtime and the number of fries in the box?
```{.default}
// Every guest comes with a different kind of food   to a potluck i.e. number
// of guests is the same as the types of food

public void divideFood(ArrayList<Food>potluck)
{
    // for each dish in the potluck
    for (Food dish : potluck)
    {
        // divide food into proper portion sizes based on guest list size.
        for (int j = 0; j < potluck.size(); j++)
        {
            // cut up the food
        }
    }
}
```
What about the algorithm above? Cutting up the food grows as the amount of food in the potluck grows? Does it grow in a linear fashion or not? Play through the scenario in your head to see if you can recognize the pattern. Two people, everything is cut into two pieces (for a total of 4 portions). Three people, everything is cut into three pieces (for a total of 9 portions). Four people, four pieces each (16 total portions).

For simple algorithms, you might be able to simply look at the algorithm and determine what its complexity is. However, more often that not you will need to carry out what is referred to as a Time analysis. It involves identifying the parts of the algorithm where repetition is carried out (since repetition is the main factor that would affect runtime especially as the input size increases) and then writing out a mathematical expression (T(n)) that describes how long a basic statement in that loop would be executed. Once the T(n) expression has been completed, determining the O() is just a matter of **identifying the most dominant term in the expression and ignoring any constants**. Let's look at a few examples of code and see if we can deduce their complexities.
```{.default}
for (int k=0; k<n; k++)
{
    ...      // How many times will this statement be executed?
}
```

Any line in the $for$ loop above will be executed n times. Therefore $T(n) = c*n$ and its complexity is O(n). The constant term c is a multiplier we use to define how long the statements in the loop would take to execute in a single iteration. What about the one below.
```{.default}
for (int k=0; k<n; k++)
{
    ...     // How many times will this statement be executed?
    for (int j=0; j<n; j++)
    {
        ...    //What about this statement?
    }
}
```
$T(n) = cn^2 + dn$    which means it is $O(n^2)$.

Those were fairly straight forward. What about these ones?

```{.default}
for (k=0; k<n/2; k++)
{ 
    ...  // This statement occurs n/2 times
    for (j=0; j<n*n; j++)
    {
        ...  //This statement occurs n*n*n/2 = n^3/2 times
    }
}
```
$T(n) = (cn^3 +dn)/2$ which means it is $O(n^3)$.

```{.default}
for (k=0; k<n/2; k++)
{
    ...  // This statement occurs n/2 times
}
for (j=0; j<n*n; j++)
{
    ...  //This statement occurs n*n = n^2 times
}
```
$T(n) = cn^2 +(dn)/2$ which means it is $O(n^2)$.

```{.default}
k = n;
while (k > 1)
{
    ...
    k /= 2;  //integer division - log2n
}
```


$T(n) = c*log_2n$ which means it is $O(log_2n)$. \
$log_2n$ implies division by 2.\
$log_3n$ implies division by 3.\
$log_4n$ implies division by 4...and so on.\
\



Sometimes calculating the actual number of steps from the T(n) expression will give you an idea of what the dominant term is based on how fast it grows with an increasing n. For example, suppose the time T (number of steps) it takes for a hypothetical algorithm to complete a problem of size n is given by the expression
        $$T(n) = 4n^2 – 2n + 2$$
Then we know that if our size is 1, it will take
        $$T(1) = 4 – 2 + 2 = 4 steps$$
If our size is 10, it will take
        $$T(10) = (4 * 10^2) – (2 * 10) + 2 = 382 steps$$
If our size is 100, it will take
        $$T(100) = (4 * 100^2) – (2 * 100) + 2 = 39802 steps$$
and if our size is 1000, it will take
        $$T(1000) = (4 * 1000^2) – (2 * 1000) + 2 = 3999802 steps$$

You will have noticed that while our input was only increasing by a factor of 10, the number of steps was increasing by a factor of 100...ish. This is because the dominant term of T(n) is $n^2$ and so we can describe the complexity of this hypothetical algorithm as being $O(n^2)$.

Once we have the big-O notation, it can then be used to provide an estimate (admittedly a very poor estimate) of the amount of time (or number of steps) that an algorithm would require for a given data size and then compare it with other algorithms without actually taking the time to implement either algorithm in code. For example given $O(n^2)$ where n is 1000, the algorithm would take approximately:
$$O(1000) ≈ 1000^2 = 1000000 steps$$

The number of steps can then later be translated to time if you know how long the average step would take on a specific system.

Below is a table showing the most common big-O categories and some common examples of algorithms in those categories [3].

+-----------+-------------+---------------------------------------------------------+
| Big-O     | Name        | Examples                                                |
+===========+=============+=========================================================+
|  $O(1)$   | constant    | - Accessing an element in an array given its index e.g. |
|           |             |   Getting the first element of a list                   |
|           |             | - Checking if a number is even or odd                   |
+-----------+-------------+---------------------------------------------------------+
| $O(logn)$ | logarithmic | - cures scurvy                                          |
|           |             | - tasty                                                 |
+-----------+-------------+---------------------------------------------------------+
| $O(n)$    | linear      | - Linear search eg.finding the maximum element in a list| 
+-----------+-------------+---------------------------------------------------------+
| $O(nlogn)$| quasi-linear| - Advanced sorting algorithms e.g. mergesort            |
+-----------+-------------+---------------------------------------------------------+
| $O(n^2)$  | Quadratic   | - Basic sorting algorithms e.g. bubblesort              |
|           |             | - Searching for duplicates in a list*                   |
|           |             | - Dealing with a two dimensional array                  |
+-----------+-------------+---------------------------------------------------------+
| $O(n^3)$  | Cubic       | - Dealing with a three dimensional array                |
|           |             | - Solving a three variable equation                     |
+-----------+-------------+---------------------------------------------------------+
| $O(2^n)$  | Exponential | - Finding all subsets of a data collection              |
|           |             | - Towers of Hanoi                                       |
+-----------+-------------+---------------------------------------------------------+
| $O(n!)$   | Factorial   | - Finding all permutations of a given data collection   |
|           |             |   e.g. figuring out a password given all the characters |
|           |             |  of the password.                                       |
+-----------+-------------+---------------------------------------------------------+





Just a few last tips on identifying the dominant term from an expression.

- n dominates $log_bn$ (b is often 2)      
- $nlog_bn$ dominates n
- $n^m$ dominates $n^k$ when m>k
- $a^n$ dominates $n^m$ for any values of a and m greater than 1.


<br>
</br>
Ref:<br>
Algorithms to Live By, by Brian Christian and Tom Griffiths
https://vickylai.com/verbose/a-coffee-break-introduction-to-time-complexity-of-algorithms/
https://adrianmejia.com/blog/2018/04/05/most-popular-algorithms-time-complexity-every-programmer-should-know-free-online-tutorial-course/
Object Oriented Data Structures using Java (4th Edition). Nell Dale, Daniel T. Joyce, Chip Weems.